{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorium 4\n",
    "## Singular Value Decomposition\n",
    "Natalia Brzozowska"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie 1. Wyszukiwarka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Ze strony internetowej https://ebible.org/find/details.php?id=eng-web&all=1 pobrałam 1402 rozdziały Biblii w formacie txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Okresl słownik słów kluczowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dictionary():\n",
    "    files = os.listdir('Bible_in_chapters/')\n",
    "    dictionary={}\n",
    "    for file in files:\n",
    "        f = io.open('Bible_in_chapters/'+file,encoding=\"utf8\")\n",
    "        wholeFile = f.read()\n",
    "        f.close()\n",
    "        sentences=wholeFile.split('\\n')\n",
    "        \n",
    "        for i in range(len(sentences)):\n",
    "            sentences[i] = sentences[i].lower()\n",
    "            sentences[i] = re.sub(r'[^\\w\\s]','',sentences[i])\n",
    "            sentences[i] = re.sub('[0-9]','',sentences[i])\n",
    "\n",
    "            words=nltk.word_tokenize(sentences[i])\n",
    "            for word in words:\n",
    "                if word in dictionary.keys():\n",
    "                    dictionary[word]+=1\n",
    "                else:\n",
    "                    dictionary[word]=1\n",
    "    \n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionary = make_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_words(diction):\n",
    "    listTuples = sorted(diction.items() ,  key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(len(diction))\n",
    "\n",
    "    for elem in listTuples :\n",
    "        print(elem[0] , \" ::\" , elem[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_words(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Dla kazdego dokumentu j wyznacz wektor cech bag-of-words dj zawierajacy czestosci wystepowania poszczególnych słów\n",
    "    (termów) w tekscie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vector(file):\n",
    "    words_freq={}\n",
    "    for word in dictionary.keys():\n",
    "        words_freq[word]=0\n",
    "    f = io.open(file,encoding=\"utf8\")\n",
    "    wholeFile = f.read()\n",
    "    f.close()\n",
    "    sentences=wholeFile.split('\\n')\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = sentences[i].lower()\n",
    "        sentences[i] = re.sub(r'[^\\w\\s]','',sentences[i])\n",
    "        sentences[i] = re.sub('[0-9]','',sentences[i])\n",
    "        words=nltk.word_tokenize(sentences[i])\n",
    "        for word in words:\n",
    "            words_freq[word]+=1\n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. Zbuduj rzadka macierz wektorów cech term-by-document matrix w której wektory cech ułozone sa kolumnowo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrix():\n",
    "    files = os.listdir('Bible_in_chapters/')\n",
    "    matrix = []\n",
    "    vec={}\n",
    "    for file in files:\n",
    "        name = 'Bible_in_chapters/'+file\n",
    "        vec=make_vector(name)\n",
    "        matrix.append(vec)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_by_doc = make_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(term_by_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5. Przetwórz wstepnie otrzymany zbiór danych mnozac elementy bag-of-words przez inverse document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_words(term_by_doc[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds in how many documents the word occurs \n",
    "def word_in_docs(w):\n",
    "    count=0\n",
    "    for p in range(len(term_by_doc)): \n",
    "        arr=term_by_doc[p]\n",
    "        if arr.get(w)!=0:\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "    \n",
    "def idf():\n",
    "    keys=dictionary.keys()\n",
    "    N=int(len(dictionary))\n",
    "    for el in keys:\n",
    "        nw=int(word_in_docs(el))\n",
    "        wsk=float(math.log(N/nw))\n",
    "        for p in range(len(term_by_doc)):\n",
    "            line=term_by_doc[p]\n",
    "            line[el]=line.get(el)*wsk\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_words(term_by_doc[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6. Napisz program pozwalajacy na wprowadzenie zapytania (w postaci sekwencji słów) przekształcanego nastepnie do reprezentacji wektorowej q (bag-of-words). Program ma zwrócic k dokumentów najbardziej zblizonych do podanego zapytania q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_me(string,k):\n",
    "    new_vec={}\n",
    "    for word in dictionary.keys():\n",
    "        new_vec[word]=0\n",
    "    \n",
    "    string = string.lower()\n",
    "    string = re.sub(r'[^\\w\\s]','',string)\n",
    "    string = re.sub('[0-9]','',string)\n",
    "\n",
    "    words=nltk.word_tokenize(string)\n",
    "    for word in words:\n",
    "            new_vec[word]+=1\n",
    "    \n",
    "    q=np.array(new_vec.values())\n",
    "#     qnorm=linalg.norm(np.array(new_vec.values()))\n",
    "    arr=[]\n",
    "    for i in range(len(term_by_doc)):\n",
    "        d_vec=np.array(term_by_doc[i].values())\n",
    "        tup=(np., i)#trzeb sie dowiedziec co to korelacja XDDDDDDDDDDDDDDDDDDD\n",
    "    \n",
    "    sorted(arr)\n",
    "    files = os.listdir('Bible_in_chapters/')\n",
    "    for i in range(k):\n",
    "        print(files[arr[i][2]])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_me(\"dont be afraid\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
